{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc1b3a4f-c0e7-484f-83c2-c904bd04eb1d",
   "metadata": {},
   "source": [
    "## 001 - 01  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d962cd-1582-44e4-a77a-b692942dda54",
   "metadata": {},
   "source": [
    "![Vogon Poetry - Zero Copy data processing over Columnar layouts](./images/vogon-poetry-v02.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c7e41-c587-405d-aff7-a55c724cba33",
   "metadata": {},
   "source": [
    "#### Columnar Layout  \n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;Array of Structs  \n",
    "#### Struct of Arrays  \n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;Metadata Medatata  \n",
    "#### Zero Copy  \n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;Distributed  \n",
    "#### Relational   \n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;Data Engineering    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e399278d-ac76-4ed9-a78b-60d7e44e9585",
   "metadata": {},
   "source": [
    "# **Vogon Poetry**   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54f5f0-3b94-441f-8a1c-ba5373cbcfdf",
   "metadata": {},
   "source": [
    "## **tl;dr**                              \n",
    "                             \n",
    "### Spark (columnar execution vs row materialization)                              \n",
    "                          \n",
    "* Stay in DataFrame/Column expressions; avoid UDFs that force row materialization.                           \n",
    "* Prefer vectorized readers/writers (Parquet/ORC) and column pruning.                          \n",
    "* Use Arrow-based interchange only when it actually avoids conversion copies; beware that crossing boundaries (Spark <-> Python) can force materialization if types are not supported or if you request row objects.                          \n",
    "                          \n",
    "### Polars (Arrow-native columnar)                          \n",
    "                          \n",
    "* Favor lazy queries and scan_* APIs (scan_parquet, scan_ipc) to push projection/filter down.                          \n",
    "* Slicing and projection are typically metadata-only.                          \n",
    "* Be cautious with operations that require full materialization (sort, certain joins, explode) and with conversions to NumPy/Pandas that may copy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f9deac-5af0-4139-9e04-0acab46c5da0",
   "metadata": {},
   "source": [
    "# Zero-copy in columnar representations                          \n",
    "                          \n",
    "                                   \n",
    "**Zero-copy** means an operation produces a result *without duplicating underlying data buffers*,   \n",
    "typically by reusing the same memory and returning new metadata (offsets, lengths, indices, validity bitmaps) that describes a “view” of that data.                          \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b9b3f-8f74-4b45-8ac2-38536171c77b",
   "metadata": {},
   "source": [
    "                 \n",
    "Columnar context: a “DataFrame” is a set of column arrays. In a columnar engine, each column is usually represented as:                          \n",
    "                          \n",
    "* One or more contiguous value buffers (e.g., int32 values)                          \n",
    "* A validity bitmap (nulls)                          \n",
    "* Optional offsets buffer (for variable-length types like strings and lists)                          \n",
    "* Optional dictionary buffers (dictionary encoding)                          \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca6da6-ab78-4342-8597-99209486c923",
   "metadata": {},
   "source": [
    "                     \n",
    "A zero-copy operation does not duplicate those buffers. It may allocate small metadata structures (array headers, offset/length, selection vectors), but not the bulk data.                          \n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14241700-7171-4d00-90f0-2c166fac096b",
   "metadata": {},
   "source": [
    "**columnar makes zero-copy practical**                          \n",
    "                          \n",
    "Columnar layouts separate values from row structure, so many operations can be expressed as:                          \n",
    "                          \n",
    "* Slice/view on a contiguous range                          \n",
    "* Projection (choose a subset of columns)                          \n",
    "* Filter as an index/selection vector referencing positions                          \n",
    "* Dictionary remapping                          \n",
    "* Chunked arrays referencing existing chunks                          \n",
    "                          \n",
    "Row-oriented systems often interleave fields per row, so extracting a column or slicing a subset of rows tends to require copying or re-packing.                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4196b6-f390-4bda-8f78-a22b1d54ef2a",
   "metadata": {},
   "source": [
    "## Common **zero-copy** (aka metadata-only) operations                          \n",
    "\n",
    "We'll implement some of these at a \"lower-level\" so we can see the innards and grasp what's going on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f77ac9-2bd5-4126-b792-eb2b279af915",
   "metadata": {},
   "source": [
    "*(we are using PyArrow here, don't get to caught up in the details, we'll dive deep into PyArrow later, deep breaths...)*    \n",
    "....\n",
    "you're going to bug me about this, so nipitinthebud - PyArrow because:\n",
    "1. its `Table.slice` is explicitly documented as a zero-copy slice.\n",
    "2. we don't get caught up in lower level details and just focus on the concepts at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f297997-0f19-4d4b-9ac9-024caa3a2b00",
   "metadata": {},
   "source": [
    "### 0. Setup and helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76bc665-a24e-4e4b-9860-893ecb7267af",
   "metadata": {},
   "source": [
    "How the helpers help:\n",
    "* `before` is a snapshot of buffer identities (addresses) for each column in a base `pyarrow.Table`\n",
    "* `after` is another `Table` produced by an operation (slice, select, take, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb8a79f-235d-49e0-96d2-66028063d4d9",
   "metadata": {},
   "source": [
    "Also, \n",
    "1. **\"Aliased\"** means **\"shared underlying memory.\"**        \n",
    "2. **\"Buffers\"** = **\"contiguous blocks of memory\"**\n",
    "  \n",
    "  \n",
    "If two Arrow arrays (or tables) are aliased, they are not holding two separate copies of the data.    \n",
    "They are two different objects that point at the same underlying buffers in memory.  \n",
    "So if we get 2 aliases (before operation and after operation) that point to the same buffer, it means the operation was **zero-copy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c879d7fa-fe83-4c49-984c-ef8313419dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e5877-5fab-441d-9ae4-a8c92eb4ac8c",
   "metadata": {},
   "source": [
    "**7** Simple helpers.  \n",
    "**2** of those (**bold**) explain if zero-copy has occurred.  \n",
    "1. `buf_addrs(arr)`\n",
    "2. `table_buf_addrs(t)` \n",
    "3. **`show_aliasing(label, before, after)`**: prints a compact per-column count of how many output chunks match any baseline chunk fingerprint. A coarse check for buffer reuse, without printing offsets, sizes, or per-buffer role details. \n",
    "4. `_buf_info(arr)` \n",
    "5. `_chunk_sig(arr)` \n",
    "6. `snapshot_table_buffers(t)` \n",
    "7. **`show_more_aliasing_info(label, before_snap, after_table, max_chunks_print):`**: More detailed comparision between the output table's underlying buffer addresses against a baseline snapshot to detect buffer reuse (aliasing). Helps us clearly see which operations are zero-copy views vs materializing new buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b0bbac-bfe5-4c93-bcd6-e2b69560f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buf_addrs(arr) collects the raw memory addresses for each physical buffer backing a single Arrow Array (one chunk).\n",
    "# A quick identity fingerprint to detect whether two arrays share the same underlying memory.\n",
    "\n",
    "def buf_addrs(arr):\n",
    "    # Return a tuple of raw memory addresses for the buffers that back one Arrow Array (one chunk)\n",
    "    addrs = []\n",
    "    # arr.buffers() returns the physical buffers for the array (validity, offsets, values, etc.)\n",
    "    for b in arr.buffers():\n",
    "        # A buffer can be None when that physical component is not present (e.g., no validity bitmap)\n",
    "        if b is None:\n",
    "            addrs.append(None)\n",
    "        else:\n",
    "            # Use the buffer address as a quick identity check for whether two arrays share memory\n",
    "            addrs.append(int(b.address))\n",
    "    # Use a tuple so it can be compared and used in sets if needed\n",
    "    return tuple(addrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805f4d31-dea0-4196-8bcd-ba70b9668d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_buf_addrs(t) builds a per-column list of per-chunk buffer-address fingerprints for an Arrow Table.\n",
    "# It lets you compare whole tables by checking whether output chunks reuse buffer addresses from an input snapshot.\n",
    "\n",
    "def table_buf_addrs(t):\n",
    "    # Build a per-column list of per-chunk buffer-address tuples for a Table\n",
    "    out = {}\n",
    "    for name in t.column_names:\n",
    "        # Tables store each column as a ChunkedArray, so we snapshot each chunk separately\n",
    "        out[name] = [buf_addrs(chunk) for chunk in t[name].chunks]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72eab05e-8415-41c8-86ca-6a83743032b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_aliasing(label, before, after) prints a compact per-column count of how many output chunks match any baseline chunk fingerprint.\n",
    "# A fast, *coarse* check for buffer reuse, without printing offsets, sizes, or per-buffer role details.\n",
    "\n",
    "def show_aliasing(label, before, after):\n",
    "    # Print how many output chunks reuse buffer addresses from the baseline snapshot\n",
    "    print(label)\n",
    "    for col in after.column_names:\n",
    "        # Baseline buffer-address tuples for this column (captured from the \"before\" table)\n",
    "        b = before.get(col)\n",
    "        # Output buffer-address tuples for this column (captured from the \"after\" table)\n",
    "        a = table_buf_addrs(after)[col]\n",
    "        # A chunk is counted as aliased if its full buffer-address tuple matches any baseline chunk tuple\n",
    "        aliased = sum(1 for x in a if x in b)\n",
    "        # Print a compact per-column summary of aliasing at the chunk level\n",
    "        print(col, \"aliased_chunks=\", aliased, \"after_chunks=\", len(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1782ed9d-dc1c-454e-8e76-09e551e91c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _buf_info(arr) returns detailed metadata for each buffer in one Arrow Array: buffer index, address, and size in bytes.\n",
    "# It supports more informative reporting so you can see exactly which buffers are shared vs newly allocated.\n",
    "\n",
    "def _buf_info(arr):\n",
    "    # Return per-buffer address and size for one Arrow Array (one chunk)\n",
    "    out = []\n",
    "    # Arrow arrays are built from 0..N buffers depending on type (validity, offsets, values, etc.)\n",
    "    bufs = arr.buffers()\n",
    "    for i, b in enumerate(bufs):\n",
    "        # Some buffer slots can be None (e.g., no validity bitmap if there are no nulls)\n",
    "        if b is None:\n",
    "            out.append({\"i\": i, \"addr\": None, \"size\": 0})\n",
    "        else:\n",
    "            # Use the raw memory address as an identity for \"is this the same underlying buffer\"\n",
    "            out.append({\"i\": i, \"addr\": int(b.address), \"size\": int(b.size)})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf466286-ca5c-408b-8d23-65e558223427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _chunk_sig(arr) builds a chunk signature from the array's buffers using (address, size) pairs for each buffer slot.\n",
    "# It enables stable equality checks for \"is this chunk backed by the same physical buffers as another chunk\" at a coarse level.\n",
    "\n",
    "def _chunk_sig(arr):\n",
    "    # Build a simple \"signature\" for a chunk based on its buffers (address + size for each buffer)\n",
    "    return tuple((x[\"addr\"], x[\"size\"]) for x in _buf_info(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f0bb3b-4636-46da-9596-d0a66f683bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snapshot_table_buffers(t) captures a reusable baseline of per-column, per-chunk metadata: type, length, offset, null_count, and buffer info/signature.\n",
    "# It provides the reference point for later comparisons so you can determine whether subsequent operations reused or replaced buffers.\n",
    "\n",
    "def snapshot_table_buffers(t):\n",
    "    # Capture a snapshot of buffer identities and key metadata for every column chunk in a Table\n",
    "    snap = {}\n",
    "    for name in t.column_names:\n",
    "        # Get the column as a ChunkedArray (Tables store each column as chunked)\n",
    "        col = t[name]\n",
    "        # Normalize into a list of chunks even if it is a single Array\n",
    "        chunks = col.chunks if isinstance(col, pa.ChunkedArray) else [col]\n",
    "        # Store per-chunk buffer signatures so we can compare \"before\" vs \"after\"\n",
    "        snap[name] = []\n",
    "        for c in chunks:\n",
    "            # Record type and view metadata because slices can alias buffers with different offsets/lengths\n",
    "            snap[name].append({\n",
    "                \"type\": str(c.type),\n",
    "                \"length\": int(len(c)),\n",
    "                \"offset\": int(c.offset),\n",
    "                \"null_count\": int(c.null_count),\n",
    "                # Store per-buffer info for more detailed inspection/debugging\n",
    "                \"buffers\": _buf_info(c),\n",
    "                # Store a compact signature for quick alias checks\n",
    "                \"sig\": _chunk_sig(c),\n",
    "            })\n",
    "    return snap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74506446-2f56-437c-b883-b0f2eb45d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_more_aliasing_info compares the output table's underlying buffer addresses against a baseline snapshot to detect buffer reuse (aliasing).\n",
    "# It prints per-column and per-chunk details (addresses, sizes, offsets) so you can see which operations are zero-copy views vs materializing new buffers.\n",
    "\n",
    "def show_more_aliasing_info(label, before_snap, after_table, max_chunks_print=4):\n",
    "    # Take a snapshot of the output table so we can compare its buffers to the baseline snapshot\n",
    "    after_snap = snapshot_table_buffers(after_table)\n",
    "\n",
    "    # Print a high-level header for this comparison\n",
    "    print(label)\n",
    "    print(\"columns\", len(after_table.column_names), \"rows\", after_table.num_rows)\n",
    "\n",
    "    # Accumulators for a summary at the end\n",
    "    total_cols = 0\n",
    "    total_after_chunks = 0\n",
    "    total_aliased_chunks = 0\n",
    "    total_new_bytes = 0\n",
    "\n",
    "    for name in after_table.column_names:\n",
    "        total_cols += 1\n",
    "\n",
    "        # Base chunks are the \"before\" buffer signatures for this column\n",
    "        b_chunks = before_snap.get(name, [])\n",
    "        # Output chunks are the \"after\" buffer signatures for this column\n",
    "        a_chunks = after_snap.get(name, [])\n",
    "        # Use a set for O(1) lookup when checking if an output chunk matches any base chunk\n",
    "        b_sigs = set(x[\"sig\"] for x in b_chunks)\n",
    "\n",
    "        aliased = 0\n",
    "        new_bytes = 0\n",
    "\n",
    "        for ac in a_chunks:\n",
    "            total_after_chunks += 1\n",
    "            # If the entire chunk signature matches, we treat it as buffer-aliased to the baseline\n",
    "            if ac[\"sig\"] in b_sigs:\n",
    "                aliased += 1\n",
    "                total_aliased_chunks += 1\n",
    "            else:\n",
    "                # If the chunk signature is new, estimate how many bytes exist in its buffers\n",
    "                # This is a crude upper bound for \"new bytes\" because some buffers may be shared elsewhere\n",
    "                for bi in ac[\"buffers\"]:\n",
    "                    if bi[\"addr\"] is not None:\n",
    "                        new_bytes += bi[\"size\"]\n",
    "\n",
    "        total_new_bytes += new_bytes\n",
    "\n",
    "        # Column-level summary\n",
    "        print(\"col\", name, \"type\", a_chunks[0][\"type\"] if a_chunks else \"NA\")\n",
    "        print(\"  chunks_after\", len(a_chunks), \"chunks_aliased\", aliased, \"new_bytes_est\", new_bytes)\n",
    "\n",
    "        # Print up to max_chunks_print chunks so the output stays readable on large chunk counts\n",
    "        nprint = min(len(a_chunks), max_chunks_print)\n",
    "        for i in range(nprint):\n",
    "            c = a_chunks[i]\n",
    "            # Offset is important: slices usually keep the same buffers but change the logical offset\n",
    "            print(\"  chunk\", i, \"len\", c[\"length\"], \"offset\", c[\"offset\"], \"nulls\", c[\"null_count\"])\n",
    "            for bi in c[\"buffers\"]:\n",
    "                # Provide a best-effort label for common buffer roles\n",
    "                role = \"buf\"\n",
    "                # Buffer 0 is typically validity bitmap when present\n",
    "                if bi[\"i\"] == 0:\n",
    "                    role = \"validity\"\n",
    "                # For variable-width types, buffer 1 is often offsets and buffer 2 is values\n",
    "                elif bi[\"i\"] == 1 and len(c[\"buffers\"]) == 3:\n",
    "                    role = \"offsets\"\n",
    "                # For fixed-width types, buffer 1 is typically values\n",
    "                elif bi[\"i\"] == 1 and len(c[\"buffers\"]) == 2:\n",
    "                    role = \"values\"\n",
    "                # For variable-width types, buffer 2 is typically values\n",
    "                elif bi[\"i\"] == 2:\n",
    "                    role = \"values\"\n",
    "                # Print raw address and size so you can see which buffers are shared vs new\n",
    "                print(\"    \", role, \"addr\", bi[\"addr\"], \"size\", bi[\"size\"])\n",
    "\n",
    "        # Indicate when we are truncating chunk output for readability\n",
    "        if len(a_chunks) > nprint:\n",
    "            print(\"  ...\", len(a_chunks) - nprint, \"more chunks omitted\")\n",
    "\n",
    "    # Overall summary across the whole table\n",
    "    print(\"summary\")\n",
    "    print(\"  total_columns\", total_cols)\n",
    "    print(\"  total_after_chunks\", total_after_chunks)\n",
    "    print(\"  total_aliased_chunks\", total_aliased_chunks)\n",
    "    print(\"  total_new_bytes_est\", total_new_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c384379-1d73-441e-882e-add07334c4d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ba3efdf-6a80-4a93-9ff2-1e2a976674aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b27004ff-443f-4ff7-a832-1f9d1d005519",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64ffab8b-b7f1-4737-8b52-7a88bef63a58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e42e4b86-e1ca-4dc6-8159-c4360db97b94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "768c95bf-4788-45b5-8755-4fd687fb8ec1",
   "metadata": {},
   "source": [
    "### 1. Projection (select subset of columns)                          \n",
    "Projection is typically metadata-only: a new schema/column list referencing existing buffers.                          \n",
    "* Selecting a subset of columns: result shares the same column buffers.                          \n",
    "* Cost: create a new schema/column list.                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7857297-fa56-41ba-82a0-43069772a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pa.table(\n",
    "    {\n",
    "        \"id\": pa.array(range(10), type=pa.int32()),\n",
    "        \"s\": pa.array([f\"v{i%3}\" for i in range(10)]),\n",
    "        \"x\": pa.array([i * 10 for i in range(10)], type=pa.int32()),\n",
    "    }\n",
    ")\n",
    "base = table_buf_addrs(t)\n",
    "base_snap = snapshot_table_buffers(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59886f8b-75fa-4b1c-80f0-cacf3dad2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base, '----------', base_snap, '----------', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2986027-bd83-4367-9cca-9e8069e935f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projection\n",
      "id aliased_chunks= 1 after_chunks= 1\n",
      "s aliased_chunks= 1 after_chunks= 1\n"
     ]
    }
   ],
   "source": [
    "p = t.select([\"id\", \"s\"])\n",
    "show_aliasing(\"projection\", base, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9249dc-889c-452d-a692-8424e460bcfc",
   "metadata": {},
   "source": [
    "### 2. Slicing (contiguous row range)                          \n",
    "                          \n",
    "* Return a view with (offset, length) into each column’s buffers.                          \n",
    "* For fixed-width types, often pure metadata.                          \n",
    "* For variable-width types (strings/lists), still typically metadata: adjust the logical offset into offsets buffer; values buffer is reused.                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "446e32f5-e4fc-41bb-bb9b-8fbea9c442c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice\n",
      "id aliased_chunks= 1 after_chunks= 1\n",
      "s aliased_chunks= 1 after_chunks= 1\n",
      "x aliased_chunks= 1 after_chunks= 1\n"
     ]
    }
   ],
   "source": [
    "sl = t.slice(2, 5)\n",
    "show_aliasing(\"slice\", base, sl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678887b4-6f78-493a-b616-60212a2dd0e0",
   "metadata": {},
   "source": [
    "### 3. Renaming / schema changes                          \n",
    "                          \n",
    "* Metadata-only.                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ee1280b-bb84-4d58-99a5-f5d01b31b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rn = t.rename_columns([\"id2\", \"s2\", \"x2\"])\n",
    "md = t.replace_schema_metadata({b\"owner\": b\"eng\", b\"purpose\": b\"bench\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1be4d9cd-463c-4db2-acd0-f53b7ff9b438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.Table\n",
       " id2: int32\n",
       " s2: string\n",
       " x2: int32\n",
       " ----\n",
       " id2: [[0,1,2,3,4,5,6,7,8,9]]\n",
       " s2: [[\"v0\",\"v1\",\"v2\",\"v0\",\"v1\",\"v2\",\"v0\",\"v1\",\"v2\",\"v0\"]]\n",
       " x2: [[0,10,20,30,40,50,60,70,80,90]],\n",
       " pyarrow.Table\n",
       " id: int32\n",
       " s: string\n",
       " x: int32\n",
       " ----\n",
       " id: [[0,1,2,3,4,5,6,7,8,9]]\n",
       " s: [[\"v0\",\"v1\",\"v2\",\"v0\",\"v1\",\"v2\",\"v0\",\"v1\",\"v2\",\"v0\"]]\n",
       " x: [[0,10,20,30,40,50,60,70,80,90]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rn, md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ccf1e-917e-4f13-b830-f9521adfc31c",
   "metadata": {},
   "source": [
    "### 4. Casting that is representation-compatible                          \n",
    "                          \n",
    "* Example: int32 to “date32” if stored as int32 days; same bits, new logical type.                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9318823-b23b-4ac9-a8d6-a43ee81896f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pa.array([1, 2, 3, 4], type=pa.int32())\n",
    "validity_buf, data_buf = a.buffers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a71574cf-4b36-4135-90aa-5fc8f558d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pa.Array.from_buffers(\n",
    "    pa.date32(),\n",
    "    len(a),\n",
    "    [validity_buf, data_buf],\n",
    "    null_count=a.null_count,\n",
    "    offset=a.offset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b9cf437-b530-4259-b7c8-3bab439abdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32_buffers [None, 3215510733184]\n",
      "date32_buffers [None, 3215510733184]\n"
     ]
    }
   ],
   "source": [
    "print(\"int32_buffers\", [int(b.address) if b else None for b in a.buffers()])\n",
    "print(\"date32_buffers\", [int(b.address) if b else None for b in date.buffers()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da87b932-d729-4369-8657-211751b3d6f2",
   "metadata": {},
   "source": [
    "### 5. Dictionary encoding reuse                          \n",
    "                          \n",
    "* If you keep codes and dictionary stable and only remap metadata.                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "122c7cb1-d8b4-4541-a386-b8d7d7e0f5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary<values=string, indices=int32, ordered=0>\n"
     ]
    }
   ],
   "source": [
    "s = pa.array([\"a\", \"b\", \"a\", \"c\", \"b\", \"a\"])\n",
    "d = pc.dictionary_encode(s)\n",
    "print(d.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3995cf1-c6b7-4182-97e7-c91e25dd6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = d.indices\n",
    "dict_values = d.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c903d084-2c6d-4ff5-a848-4b3eaab69313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codes_alias True\n",
      "dict_alias True\n"
     ]
    }
   ],
   "source": [
    "d2 = pa.DictionaryArray.from_arrays(codes, dict_values)\n",
    "print(\"codes_alias\", buf_addrs(codes) == buf_addrs(d2.indices))\n",
    "print(\"dict_alias\", buf_addrs(dict_values) == buf_addrs(d2.dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79bbef89-1f3d-4cc1-a177-80641e4c2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = pa.scalar(1, type=pa.int32())\n",
    "zero = pa.scalar(0, type=pa.int32())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db99296a-4219-472b-b5e5-7855b7ac7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = pc.equal(pc.bit_wise_and(t[\"id\"], one), zero)\n",
    "idx = pc.indices_nonzero(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09290a9d-43c0-4b59-951d-de0bb9649bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logical = (t, idx)  # table + selection vector\n",
    "phys = t.take(idx)  # materialized compact result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b86362e-3f7c-4c3f-b8e4-8f6dc9bc67fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logical_indices_type uint64\n",
      "filter_materialize_take\n",
      "id aliased_chunks= 0 after_chunks= 1\n",
      "s aliased_chunks= 0 after_chunks= 1\n",
      "x aliased_chunks= 0 after_chunks= 1\n"
     ]
    }
   ],
   "source": [
    "print(\"logical_indices_type\", idx.type)\n",
    "show_aliasing(\"filter_materialize_take\", base, phys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "975a86a6-abd7-4511-bd7c-68fe5cc078eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = pc.sort_indices(t, sort_keys=[(\"s\", \"ascending\"), (\"id\", \"descending\")])\n",
    "sorted_t = t.take(perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c919de4b-3fdb-44c9-a918-290e4e670e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perm_type uint64\n",
      "sort_materialize_take\n",
      "id aliased_chunks= 0 after_chunks= 1\n",
      "s aliased_chunks= 0 after_chunks= 1\n",
      "x aliased_chunks= 0 after_chunks= 1\n"
     ]
    }
   ],
   "source": [
    "print(\"perm_type\", perm.type)\n",
    "show_aliasing(\"sort_materialize_take\", base, sorted_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2244ac3-55b4-4052-82b0-021db03c61cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pa.table({\"k\": [1, 2, 3], \"lv\": [\"a\", \"b\", \"c\"]})\n",
    "right = pa.table({\"k\": [2, 3, 4], \"rv\": [\"B\", \"C\", \"D\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb07054e-d678-4c6a-8242-4e559677b442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "k: int64\n",
      "lv: string\n",
      "rv: string\n",
      "----\n",
      "k: [[2,3,1]]\n",
      "lv: [[\"b\",\"c\",\"a\"]]\n",
      "rv: [[\"B\",\"C\",null]]\n"
     ]
    }
   ],
   "source": [
    "j = left.join(right, keys=\"k\")\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd8a534d-5ca8-4926-9c92-09c237b618c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "s: string\n",
      "x_sum: int64\n",
      "id_count: int64\n",
      "----\n",
      "s: [[\"v0\",\"v1\",\"v2\"]]\n",
      "x_sum: [[180,120,150]]\n",
      "id_count: [[4,3,3]]\n"
     ]
    }
   ],
   "source": [
    "g = t.group_by(\"s\").aggregate([(\"x\", \"sum\"), (\"id\", \"count\")])\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "710e85e8-f835-45eb-95b7-0f3d1a69e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = t.slice(0, 5)\n",
    "t2 = t.slice(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d209211-6330-4b61-b0f7-354561b08035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "cat = pa.concat_tables([t1, t2], promote_options=\"none\")\n",
    "print([len(t1[\"id\"].chunks), len(t2[\"id\"].chunks), len(cat[\"id\"].chunks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74ec3a19-4080-4ea5-9994-01e405dedefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1]\n"
     ]
    }
   ],
   "source": [
    "compact = cat.combine_chunks()\n",
    "print([len(cat[\"id\"].chunks), len(compact[\"id\"].chunks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69012861-5400-476b-853e-33e1aad04875",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = t.slice(1, 3)  # zero-copy buffers, new metadata object :contentReference[oaicite:12]{index=12}\n",
    "idx = pc.indices_nonzero(pc.equal(t[\"id\"], 3))  # allocates indices buffer, no base buffer rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49168403-505f-4bdd-9fb4-0bfe630e0f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logical = (t, perm)      # logical sorted view\n",
    "physical = t.take(perm)  # physical sorted table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37faff-87c3-4020-bcc9-b0d75238859d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e57b8-a593-4712-9717-ce89706c82ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d102b2ad-80dc-45d5-a516-92d864e5ff43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083d8ec-28f9-4b5d-aec9-1e2f407bebfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
